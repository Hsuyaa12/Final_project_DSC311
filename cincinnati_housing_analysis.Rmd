DATASET

CrimeRate - Per capita crime rate by city
Indus - Proportion of non-retail business acres per city.
River – River front or not (1-bounds river; 0-otherwise)
AvgRoom - Average number of rooms per dwelling per city
Age – Average age of a house by city
Tax - full-value property-tax rate per $10,000
PTRatio - pupil-teacher ratio by city
LStat - % lower status of the population by city
MedPrice - Median price of owner-occupied homes in $1000's

Load necessary libraries
------------------------
``` {r}
library(tidyverse)
library(ggplot2)
library(GGally)
library(leaps)
library(MASS)
library(car)
library(nortest)
library(gridExtra)
```

Ingest data
------------------------
```{r}
cincinnati <- read_csv("CINCINNATI.csv")
head(cincinnati)
```


Data cleaning
------------------------
```{r}
#Check for missing values
any(is.na(cincinnati))

#Check for duplicates
colSums(is.na(cincinnati))

#Check for correct format
str(cincinnati$CrimeRate)
str(cincinnati$Indus)
str(cincinnati$River)
str(cincinnati$AvgRoom)
str(cincinnati$Age)
str(cincinnati$Tax)
str(cincinnati$PTRatio)
str(cincinnati$LStat)
str(cincinnati$MedPrice)

#Check for inconsistencies/invalid values
any(cincinnati$CrimeRate < 0)
any(cincinnati$Indus <= 0)
any(cincinnati$River < 0 | cincinnati$River > 1)
any(cincinnati$AvgRoom < 0)
any(cincinnati$Age < 0)
any(cincinnati$Age <= 0)
any(cincinnati$Tax <= 0)
any(cincinnati$PTRatio <= 0)
any(cincinnati$PTRatio <= 0)
any(cincinnati$LStat < 0 | cincinnati$LStat > 100)
any(cincinnati$MedPrice <= 0)

#Look for outliers, determine how to handle them
summary(cincinnati$CrimeRate)
boxplot(cincinnati$CrimeRate, main = "Boxplot of Crime Rate")
hist(cincinnati$CrimeRate, breaks = 20, col = "lightcoral", main = "Histogram of Crime Rate")
#Conclusion: There are two high crimerate outliers, but they are still reasonable, and without knowing which city the crime rate is associated with, there's no way to investigate to see if we can justify removing it 

summary(cincinnati$Indus)
boxplot(cincinnati$Indus, main = "Boxplot of Porportion of Industry")
hist(cincinnati$Indus, breaks = 20, col = "lightcoral", main = "Histogram of Propotion of Industry")
#Conclusion: No outliers

summary(cincinnati$AvgRoom)
boxplot(cincinnati$AvgRoom, main = "Boxplot of Average Number of Rooms")
hist(cincinnati$AvgRoom, breaks = 20, col = "lightcoral", main = "Histogram of Average Number of Rooms")
#Conclusion: No outliers

summary(cincinnati$Age)
boxplot(cincinnati$Age, main = "Boxplot of Average Number of Rooms")
hist(cincinnati$Age, breaks = 20, col = "lightcoral", main = "Histogram of Average Number of Rooms")
#Conclusion: No outliers

summary(cincinnati$Tax)
boxplot(cincinnati$Tax, main = "Boxplot of Tax Rate")
hist(cincinnati$Tax, breaks = 20, col = "lightcoral", main = "Histogram of Tax Rate")
outlier <- subset(cincinnati, Tax == 666)
outlier
#Conclusion: Initially it looks like there is an outlier on 666, but upon further examination, there are 36 records with this value, meaning it's not an outlier. Need to figure out why this is here. 

summary(cincinnati$PTRatio)
boxplot(cincinnati$PTRatio, main = "Boxplot of Pupil-teacher ratio")
hist(cincinnati$PTRatio, breaks = 20, col = "lightcoral", main = "Histogram of Pupil-teacher ratio")
#Conclusion: One entry is a bit low, but not an outlier

summary(cincinnati$LStat)
boxplot(cincinnati$LStat, main = "Boxplot of Lower Status Proportion")
hist(cincinnati$LStat, breaks = 20, col = "lightcoral", main = "Histogram of Lower Status Proportion")
#Conclusion: No outliers

summary(cincinnati$MedPrice)
boxplot(cincinnati$MedPrice, main = "Boxplot of Median House Price")
hist(cincinnati$MedPrice, breaks = 20, col = "lightcoral", main = "Histogram of Median House Price")
#Conclusion: No outliers
```


Exploratory Analysis
------------------------
```{r}
#Analyze relationship between variables
ggpairs(cincinnati)
ggcorr(cincinnati, label = TRUE, label_round = 2, palette = "RdYlBu")
#Biggest impact on Median Home Price:
#-AvgRooms 0.747 (More rooms = more $)
#-LStat -0.751 (Lower status = less $)
#-PTRatio -0.575 (higher ratio = less teachers/student = less $)
#-Indus -0.574 (more non-retail business = less appealing to live at = less $)
#-Tax -0.540 (higher tax rate = lower $)
#-Age -0.482 (the older the house = less $)

#Possible Variable interdependency
#-Tax/CrimeRate 0.538
#-Tax/Indus 0.698
#-Age/Indus 0.671
#-LStat/Indus 0.645
#-LStat/AvgRoom -0.615
#-Tax/Age  0.508
#-Age/LStat 0.654
#-LStat/Tax 0.565


#Check linear regression assumption of linearity
ggplot(cincinnati, aes(x = CrimeRate, y = MedPrice)) + 
  geom_point() +  
  labs(title = "Crimerate vs Price",
       x = "Crimerate",
       y = "Price") +
  theme_minimal()             
#Result: Negative non-linear

ggplot(cincinnati, aes(x = River, y = MedPrice)) + 
  geom_point() +  
  labs(title = "River vs Price",
       x = "River",
       y = "Price") +
  theme_minimal()             
#Result: Small negative correlation for on river

ggplot(cincinnati, aes(x = AvgRoom, y = MedPrice)) + 
  geom_point() +  
  labs(title = "AvgRoom vs Price",
       x = "AvgRoom",
       y = "Price") +
  theme_minimal()             
#Result: Clear Positive Linear 

ggplot(cincinnati, aes(x = Age, y = MedPrice)) + 
  geom_point() +  
  labs(title = "Age vs Price",
       x = "Age",
       y = "Price") +
  theme_minimal()             
#Result: Negative Non-linear 

ggplot(cincinnati, aes(x = Tax, y = MedPrice)) + 
  geom_point() +  
  labs(title = "Tax vs Price",
       x = "Tax",
       y = "Price") +
  theme_minimal()             
#Result: General Negative Correlation, but a weird grouping at the high end of specific tax rate

ggplot(cincinnati, aes(x = PTRatio, y = MedPrice)) + 
  geom_point() +  
  labs(title = "PTRatio vs Price",
       x = "PTRatio",
       y = "Price") +
  theme_minimal()             
#Result: Loose linear negative

ggplot(cincinnati, aes(x = LStat, y = MedPrice)) + 
  geom_point() +  
  labs(title = "LStat vs Price",
       x = "LStat",
       y = "Price") +
  theme_minimal()             
#Result: Non-linear negative

```


```{r}
library(dplyr); library(tidyr); library(knitr)

demographics <- cincinnati %>%
  summarise_if(
    is.numeric,
    list(
      Mean   = ~mean(.x, na.rm = TRUE),
      Median = ~median(.x, na.rm = TRUE),
      SD     = ~sd(.x, na.rm = TRUE),
      Min    = ~min(.x, na.rm = TRUE),
      Max    = ~max(.x, na.rm = TRUE)
    )
  ) %>%
 
  pivot_longer(
    everything(),
    names_to  = c("Variable", "Statistic"),
    names_sep = "_",
    values_to = "Value"
  ) %>%
  pivot_wider(
    names_from  = Statistic,
    values_from = Value
  )

kable(demographics, digits = 2,
      caption = "Descriptive Statistics for Key Variables")

```


```{r demography-r}

# count how many houses are riverfront vs. non-riverfront
river_counts <- cincinnati %>%
  group_by(River) %>%
  summarise(
    count = n(),
    pct   = round(n() / nrow(cincinnati) * 100, 1)
  )
river_counts


```
Findings: Only 7 houses are riverfront which is only about 4.6% . Doing further analysis to see if we can drop it.

```{r demography-river}
# Checking to see if riverfront houses are outliers
#  Compute IQR thresholds for MedPrice
Q1      <- quantile(cincinnati$MedPrice, 0.25)
Q3      <- quantile(cincinnati$MedPrice, 0.75)
IQR_val <- IQR(cincinnati$MedPrice)
lower   <- Q1 - 1.5 * IQR_val
upper   <- Q3 + 1.5 * IQR_val


cincinnati <- cincinnati %>%
  mutate(is_outlier = MedPrice < lower | MedPrice > upper)

# See how many outliers are by river status
outlier_summary <- cincinnati %>%
  filter(is_outlier) %>%
  group_by(River) %>%
  summarise(
    n = n(),
    pct = round(n()/nrow(cincinnati)*100, 1)
  )
print(outlier_summary)

```

```{r demography-river outlier}

ggplot(cincinnati, aes(
  x = factor(River, labels = c("Non-river", "Riverfront")),
  y = MedPrice
)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16) +
  labs(
    title = "Median Home Price by River Status",
    x     = "River Status",
    y     = "Median Price ($1000s)"
  ) +
  theme_minimal()

```

# Findings:
All the really low and really high prices outliers come from non-river homes. Riverfront homes all sit in the normal price range. So we can't drop river front houses although their portion in our dataset is not significant.

Research Question Investigation
------------------------
```{r}
#How does the crime rate affect housing prices across different price segments?
#create price quartiles
cincinnati$price_quartile <- cut(cincinnati$MedPrice, 
                                 breaks = quantile(cincinnati$MedPrice, probs = 0:4/4), 
                                 include.lowest = TRUE, 
                                 labels = c("Q1", "Q2", "Q3", "Q4"))

# calculate mean crime rate for each quartile
mean_crime_by_quartile <- cincinnati %>%
  group_by(price_quartile) %>%
  summarize(mean_crime_rate = mean(CrimeRate, na.rm = TRUE))

#create scatter plot with trend lines
ggplot(cincinnati, aes(x = MedPrice, y = CrimeRate)) +
  geom_point(aes(color = price_quartile), alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, aes(group = price_quartile)) +
  labs(title = "Crime Rate vs Housing Prices by Price Quartile",
       x = "Housing Price",
       y = "Crime Rate") +
  theme_minimal()


```
Interpretation:
As we can see here, it appears that crimerate has the highest impact on low price homes. We can see that the lowest price quartile (Q1) seems to have the highest crime rates, and the trend line indicates that there is a strong negative correlation with housing price and crime rate in Q1. Although there are some smal trends in the other quartiles, they aren't really clear enough to draw meaningful conclusions. From this, it appears that high crime disproportionately impacts lower price housing more than any other pricing group. 

```{r}
# Investigating the RelationShip between Housing Age and Room Number
ggplot(cincinnati, aes(x = Age, y = AvgRoom)) +
  geom_point(color = "blue") +
  labs(title = "Relationship between Housing Age and Average Room Numbers",
       x = "Housing Age (Years)",
       y = "Average Number of Rooms") +
  theme_minimal()

# Calculate correlation coefficient
age_avgroom_cor <- cor(cincinnati$Age, cincinnati$AvgRoom)
print(paste("The Pearson correlation coefficient between Age and AvgRoom is:", round(age_avgroom_cor, 3)))

cor.test(cincinnati$Age, cincinnati$AvgRoom)

```


Interpretation:
The correlation between the age and AvgRoom is -0.2423 , which means there is a mild
negative linear relationship between them and as house get older, they tend to have slightly fewer rooms on average.
Upon checking the p-value which is 0.002811 < 0.05 , we can say the observation is significantly significant.
Also the confidence interval does not contain zero, it supports that true correlation is different from zero 
and that there is a statistically significant negative relationship. 
We can conclude that, in our dataset, as the housing age increases, there is a 
slight tendency for the average number of rooms to decrease.



```{r}
#Investigating: Is there a relationship between the student-teacher ratio (PTRatio) and the crime rate (CrimeRate) in the city?
ggplot(cincinnati, aes(x = PTRatio, y = CrimeRate)) +
  geom_point(color = "blue") +
  labs(title = "Relationship between PTRatio and CrimeRate",
       x = "PTRatio",
       y = "CrimeRate") +
  theme_minimal()

#Calculate correlation coefficient
ptratio_crimerate_cor <- cor(cincinnati$PTRatio, cincinnati$CrimeRate)
print(paste("The Pearson correlation coefficient between PTRatio and CrimeRate is:", round(ptratio_crimerate_cor, 3)))

cor.test(cincinnati$PTRatio, cincinnati$CrimeRate)

```

Interpretation:
The correlation between PTRatio and Crimerate is 0.274, which suggests that as the ratio of students to teachers increases, so does the crimerate. This intuitively makes sense, because less affuent cities will have less teachers for students, and areas such as this are typically more prone to crime. 
Upon checking the p-value which is 0.00067 < 0.05 , we can say the observation is significantly significant.
Also the confidence interval does not contain zero, it supports that true correlation is different from zero and that there is a statistically significant negative relationship. 
We can conclude that, in our dataset, as the housing age increases, there is a 
slight tendency for the average number of rooms to decrease.


```{r}
#Investigating: How do socioeconomic factors (LStat variable) correlate with property tax rates?
#   LStat - % lower status of the population by city
#   Tax - full-value property-tax rate per $10,000
cincinnati <- read_csv("CINCINNATI.csv") #Reset data
ggplot(cincinnati, aes(x = Tax, y = LStat)) +
  geom_point() +
  labs(
    title = "Tax vs LStat"
  )

# Attempting to explain strange grouping found in scatter plot
#   grouping by river
ggplot(cincinnati, aes(x = Tax, y = LStat, color=as.factor(River))) +
  geom_point() +
  labs(
    title = "Tax vs LStat by River",
    color = "River"
  )

#   grouping by CrimeRate Category
cincinnati$CrimeRate <- cut(
  cincinnati$CrimeRate,
  breaks = c(0, 1, 5, Inf),    # Define breakpoints for "low", "moderate", "high"
  labels = c("low", "moderate", "high")      # Labels for each category
)
ggplot(cincinnati, aes(x = Tax, y = LStat, color=as.factor(CrimeRate))) +
  geom_point() +
  labs(
    title = "Tax vs LStat by Crime rate category",
    color = "Crime rate category"
  )

#   grouping by Indus
cincinnati$Indus <- cut(
  cincinnati$Indus,
  breaks = c(0, 13.60556, Inf), # this cutoff is explained in the next block
  labels = c("low", "high")      
)
ggplot(cincinnati, aes(x = Tax, y = LStat, color=Indus)) +
  geom_point() +
  labs(
    title = "Tax vs LStat by Indus",
    color = "Indus"
  )
```

Interpretation:
There is a positive linear relationship between Tax and LStat, in general as Tax increases LStat also increases.
However there seems to be a strange group around a value of ~675 for Tax.
Examining the group by coloring by different categorical variables:
- River doesn't seem to have any effect on the grouping, there aren't enough data points that bound the river (1) to make conclusions.
- Crime rate categories of 'moderate' and 'high' seem to correspond well with the grouping.
- Data points with high Indus levels seem to all be part of the strange grouping.


```{r}
#Investigating: What is the relationship between industrial land use (Indus) and housing characteristics such as room sizes, property tax rates, and housing prices?
cincinnati <- read_csv("CINCINNATI.csv") #Reset data
ggplot(cincinnati, aes(x=Indus))+
  geom_density()+
  labs(
    title = "Indus Density Plot",
    x = "Indus",
    y = "Density"
  )

# === Finding where to split the data ===
# Compute the density
density_values <- density(cincinnati$Indus)

# Find local minima
x <- density_values$x
y <- density_values$y

# Compare each y-value with its neighbors
local_minima <- which(diff(sign(diff(y))) == 2) + 1  # Indices of local minima


# Extract the x and y coordinates of the minima
minima_x <- x[local_minima]
minima_y <- y[local_minima]

# Print the local minima
data.frame(x = minima_x, y = minima_y)
# x          y
# 1 13.60556 0.01937223

ggplot(cincinnati, aes(x = Indus)) +
  geom_density() +
  geom_vline(xintercept = minima_x, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Indus Density Plot Split",
    x = "Indus",
    y = "Density"
  )

# === Splitting Indus -> low + high ===
cincinnati$Indus <- cut(
  cincinnati$Indus,
  breaks = c(0, minima_x, Inf),
  labels = c("low", "high")      
)

ggpairs(cincinnati[, c("Indus", "AvgRoom", "Tax", "MedPrice")], aes(color=cincinnati$Indus, alpha=0.5))

```

Interpretation:
There seems to be roughly double the amount of low Indus as high Indus,
The Indus level doesn't have much effect of on average room number (AvgRoom)
Median price (MedPrice) is shows slight change in distribution between the Indus levels. On average a house in a high Indus area will have a lower MedPrice.
Tax rate (Tax) however, varies wildly between low and high Indus levels. Tax rates in a high Indus area will on average, be much higher than those in a low area.


Data Transformations/Categorizations
--------------------------------------
```{r}
cincinnati <- read_csv("CINCINNATI.csv") #Reset data
```

Investigate bimodality in Indus, convert to categorical variable
```{r}
#There appears to be bimodality with Indus
dens <- density(cincinnati$Indus)
plot(dens, main = "Checking for Bimodality in Indus")
polygon(dens, col = "skyblue", border = "darkblue")
#Find local minima
local_mins <- which(diff(sign(diff(dens$y))) == 2) + 1
min_x_values <- dens$x[local_mins]
min_y_values <- dens$y[local_mins]
#Add vertical lines and points at the local minima
abline(v = min_x_values, col = "red", lty = 2)
points(min_x_values, min_y_values, col = "red", pch = 19)
#Print the local minimum x-values
min_x_values
#Convert Indus to a categorical variable.
cincinnati$IndusCat <- ifelse(cincinnati$Indus < min_x_values, "Low", "High")
cincinnati$IndusCat <- factor(cincinnati$IndusCat, levels = c("Low", "High"))
ggpairs(data = cincinnati, aes(color = IndusCat, alpha = 0.6))

```

Investigate weird spread of CrimeRate, convert to categorical variable
```{r}
ggplot(cincinnati, aes(x=CrimeRate))+
  geom_histogram()

summary(cincinnati$CrimeRate)
 #    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
 # 0.01096  0.07250  0.19160  3.34698  2.69719 88.97620 

#recoding crimerate
# breakpoints: (0, Q1, Q3, inf) -> low, medium, high

#cincinnati$CrimeRate <- cut(
#  cincinnati$CrimeRate,
#  breaks = c(0, 0.07250, 2.69719, Inf),    # Define breakpoints for "low", "medium", "high"
#  labels = c("low", "medium", "high")      # Labels for each category
#)
#ggpairs(cincinnati, aes(color=cincinnati$CrimeRate, alpha=0.5))


# ==== alt crimerate breakpoints ====

#cincinnati <- read.csv("CINCINNATI.csv")
#cincinnati$River <- recode(cincinnati$River, `0`="otherwise", `1`="bounds river")

# using (0, Q1, Q3, inf) as breakpoints for low, medium, high didn't seem to represent it well
# using arbitrary (0, 1, 5, inf) seems like better breakpoints

cincinnati$CrimeRateCat <- cut(
  cincinnati$CrimeRate,
  breaks = c(0, 1, 5, Inf),    # Define breakpoints for "low", "medium", "high"
  labels = c("Low", "Medium", "High")      # Labels for each category
)
cincinnati$CrimeRateCat <- factor(cincinnati$CrimeRateCat, levels = c("Low", "Medium", "High"))
ggpairs(cincinnati, aes(color=cincinnati$CrimeRateCat, alpha=0.5))

#create binary variables for categorical variable
cincinnati$IsLowCrime <- ifelse(cincinnati$CrimeRateCat == "Low", 1, 0)
cincinnati$IsMediumCrime <- ifelse(cincinnati$CrimeRateCat == "Medium", 1, 0)
cincinnati$IsHighCrime <- ifelse(cincinnati$CrimeRateCat == "High", 1, 0)


```


Individual Variable Residual Analysis
-------------------------------------
```{r}
#CrimeRate
m <- lm(MedPrice ~ CrimeRateCat, data = cincinnati)
par(mfrow = c(2,2))
plot(m)
#examine each category vs MedPrice
ggplot(cincinnati, aes(x = CrimeRate, y = MedPrice, color = CrimeRateCat)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Crime vs Median Housing Price",
       x = "Crime Rate",
       y = "Median Housing Price",
       color = "Crime Category")

```
Findings: Looks fine, nothing of note here. Confirms that higher crime rates generally correlate to lower housing prices

```{r}
#IndusCat
m <- lm(MedPrice ~ IndusCat, data = cincinnati)
par(mfrow = c(2,2))
plot(m)
#examine each category vs MedPrice
ggplot(cincinnati, aes(x = Indus, y = MedPrice, color = IndusCat)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Indus vs Median Housing Price",
       x = "Indus",
       y = "Median Housing Price",
       color = "Indus Category")
```
Findings: Looks fine, nothing of note. Confirms that higher Indus categories generally have a moderately lower housing price

```{r}
#River
m <- lm(MedPrice ~ River, data = cincinnati)
par(mfrow = c(2,2))
plot(m)
```
Findings: Looks mostly ok, the normality looks slightly skewed but not too alarming.

```{r}
#AvgRoom
m <- lm(MedPrice ~ AvgRoom, data = cincinnati)
par(mfrow = c(2,2))
plot(m)

```
Findings: Overall, it looks ok. The Residuals vs Fitted plot has a moderate funnel shape, indicating possible slight levels of non-constant varience. The trend line on the Residuals vs Fitted plot isn't too bad, so leave as is for now. Normality looks good. 

```{r}
#Age
m <- lm(MedPrice ~ Age, data = cincinnati)
par(mfrow = c(2,2))
plot(m)

#What does log transform do?
m_log <- lm(log(MedPrice) ~ Age, data = cincinnati)
par(mfrow = c(2,2))
plot(m_log)

```
Findings: Residual plot looks a bit concerning. Close grouping below residual line, spread grouping above it. Q-Q plot right side trends upward rather quickly. Doing a log transform on MedPrice seems to fix normality and tighten up the varience, though there is still a reverse funnel shape, indicating more variance at lower prices. This makes sense, since lower-priced homes are more susceptible to factors like crime, age, etc. which aren't in the model here. 

```{r}
#Tax
m <- lm(MedPrice ~ Tax, data = cincinnati)
par(mfrow = c(2,2))
plot(m)

```
Findings: We have a wierd grouping of tax rates on the left side, which corresponds to the high indus tax bracket, and then a normal spread on the right, corresponding to the low indus cities. On both sides, the residuals look evenly spread around the line, indicating equal varience despite the weird groupings. The normality also looks good. 

```{r}
#PTRatio
m <- lm(MedPrice ~ PTRatio, data = cincinnati)
par(mfrow = c(2,2))
plot(m)

```
Findings: Looks ok, nothing too noteworthy here. 

```{r}
#LStat
m <- lm(MedPrice ~ LStat, data = cincinnati)
par(mfrow = c(2,2))
plot(m)

m1 <- lm(MedPrice ~ I(LStat^2), data = cincinnati)
par(mfrow = c(2,2))
plot(m1)

m2 <- lm(MedPrice ~ I(1/LStat), data = cincinnati)
par(mfrow = c(2,2))
plot(m2)

m3 <- lm(MedPrice ~ log(LStat), data = cincinnati)
par(mfrow = c(2,2))
plot(m3)

```
Findings: Initial plot of LStat indicates non-linearity in the residual plot, and also in the Q-Q plot via a sharp deviation on the right end of the line. To test the non-linearity theory, I applied two different transformations on LStat:
1. Square LStat: Standard way to try to linearize a relationship. This did nothing to help the residual plot, and seems to have only sharpened the trend.
2. 1/LStat: This was indicated by the shape of the LStat vs MedPrice plot earlier, where there seemed to be a 1/x relationship. 

I also decided to try a log transform on LStat, which actually seemed to significantly improve the residual plot. The Q-Q plots of 1/LStat and log(LStat) seem almost the same, but the residual plot is the best for log(Lstat), indicating that doing the log transformations seems to significantly improve the variance. 


Figure out how to handle Indus/Tax relationship 
------------------------------------------------
```{r}
ggplot(cincinnati, aes(x = Tax, y = MedPrice, color = IndusCat)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Tax vs MedPrice w/IndusCat",
       x = "Tax",
       y = "MedPrice",
       color = "Indus Category")

ggplot(cincinnati, aes(x = Tax, y = MedPrice, color = IndusCat)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, color = "black", linetype = "dashed") + # Overall trendline
  theme_minimal() +
  labs(title = "Tax vs MedPrice with Trendlines (by IndusCat and Overall)",
       x = "Tax",
       y = "MedPrice",
       color = "Indus Category")

```
Findings: The special group (High Indus, Tax = 666) isn't drastically deviating in terms of the Tax relationship. The blue trendline, which includes the cluster at Tax ≈ 666, doesn't show a sharp bend or a completely different slope at that high tax value compared to the rest of the blue line. This reinforces the observation that they might roughly follow a similar trend with respect to tax. For now, I'll leave as is. 


Split Data into Training/Test sets
-----------------------------------
```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(cincinnati), replace=TRUE, prob=c(0.7,0.3))
train  <- cincinnati[sample, ]
test   <- cincinnati[!sample, ]

```



Linear Regression - Full Model
---------------------------------
```{r}
full_model1 <- lm(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + log(LStat), data = train)
summary(full_model1)
par(mfrow = c(2,2))
plot(full_model1)
ad.test(full_model1$residuals)

full_model2 <- lm(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + I(1/LStat), data = train)
summary(full_model2)
par(mfrow = c(2,2))
plot(full_model2)
ad.test(full_model2$residuals)

```

Findings: considering both the slightly better summary statistics and the potentially improved linearity suggested by the residuals plot, it appears that using I(1/LStat) is likely the better transformation for LStat in the linear regression model for predicting MedPrice in this dataset. Addtionally, the AD p-values for both models indicate that we don't have strong evidence against normality.

Linear Regression - Best Subset Model Selection
---------------------------------------------------
```{r}
best_subset_model <- regsubsets(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + I(1/LStat), data = train, nbest = 2)
best_subset_summary <- summary(best_subset_model)

#Add RSS, Adjusted R², Cp, and BIC, MSE, etc to the same data frame
mse_values <- best_subset_summary$rss / (nrow(train) - 1:nrow(best_subset_summary$which))
mse_values <- round(mse_values, 2)

results <- data.frame(
  modelId = 1:nrow(best_subset_summary$which),
  best_subset_summary$outmat,
  Rsq = best_subset_summary$rsq,
  RSS = best_subset_summary$rss,
  AdjR2 = best_subset_summary$adjr2,
  Cp = best_subset_summary$cp,
  BIC = best_subset_summary$bic,
  MSE = mse_values
)
results$Rsq <- round(results$Rsq, 2)
results$RSS <- round(results$RSS, 2)
results$AdjR2 <- round(results$AdjR2, 2)
results$Cp <- round(results$Cp, 2)
results$BIC <- round(results$BIC, 2)


print(results)
```

Make four plots to allow comparison between models
-----------------------------------------------------
```{r}
p1<-ggplot(data = results, aes(x = as.factor(modelId), y = MSE))+
  geom_point(color="black", size=2) +ylab("MSE") + xlab("ModelId")

p2<-ggplot(data = results, aes(x = as.factor(modelId), y = AdjR2)) +
  geom_point(color="black", size=2) +ylab("Adjusted RSQ") + xlab("ModelId")+
  geom_point(data = results[which.max(results$AdjR2), ], color="red", 
             size=3) 

p3<-ggplot(data = results, aes(x = as.factor(modelId), y = Cp)) +
  geom_point(color="black", size=2) +ylab("Cp") + xlab("ModelId")+
  geom_point(data = results[which.min(results$Cp), ], color="red", 
             size=3) 

p4<-ggplot(data = results, aes(x = as.factor(modelId), y = BIC)) +
  geom_point(color="black", size=2) +ylab("BIC") + xlab("ModelId")+
  geom_point(data = results[which.min(results$BIC), ], color="red", 
             size=3) 

grid.arrange(p1,p2,p3,p4,nrow=2)

```
Findings: It looks like model 7 is the clear winner. It is closely tied for lowest MSE, has the best Adjust RSQ, and lowest Cp and Bix values.

Get Model 7
-------------------
```{r}
coef(best_subset_model, 7)
model7 <- lm(MedPrice ~ CrimeRateCat + AvgRoom + PTRatio + I(1/LStat), data=train)
summary(model7)
par(mfrow = c(2,2))
plot(full_model1)
ad.test(full_model1$residuals)
anova(model7)
deviance(model7)
vif(model7)
```
Findings: The linear model fitted to the training data, predicting MedPrice using CrimeRateCat, AvgRoom, PTRatio, and I(1/LStat), demonstrates a strong overall fit, with a high Multiple R-squared of 0.7946 and a significant F-statistic (p < 2.2e-16). The ANOVA results indicate that all four predictors, CrimeRateCat, AvgRoom, PTRatio, and I(1/LStat), are statistically significant contributors to explaining variation in median housing prices.

Specifically, a higher average number of rooms and a lower percentage of lower-status population (reflected by a higher value of 1/LStat) are associated with higher median prices. A higher pupil-teacher ratio (PTRatio) is associated with lower median prices. The crime rate category also has a significant impact, with areas classified as "High" showing substantially lower median prices compared to the "Low" reference group.

The residual sum of squares for the model is 1420.17, and an Anderson-Darling test on the residuals suggests no strong evidence against normality (p = 0.4397). Finally, variance inflation factors (VIFs) are all below 2, indicating that multicollinearity is not a serious concern in the model

Linear Regression - Forward Selection
---------------------------------------
```{r}
null_model <- lm(MedPrice ~ 1, data = train)
upper_model <- formula(full_model2)
forward_model <- step(null_model, direction = "forward", scope = upper_model)
coef(forward_model)
summary(forward_model)
par(mfrow = c(2,2))
plot(forward_model)
ad.test(forward_model$residuals)
anova(forward_model)
deviance(forward_model)
vif(forward_model)

```
Findings: Same model as was produced by best subset selection (model7)


Linear Regression - Backward Selection
----------------------------------------
```{r}
backward_model <- step(full_model2, direction = "backward")
coef(backward_model)
summary(backward_model)
par(mfrow = c(2,2))
plot(backward_model)
ad.test(backward_model$residuals)
anova(backward_model)
deviance(backward_model)
vif(backward_model)
```
Findings: Also produced the exact same model as the forward selection and model7


Model Comparison & Analysis
```{r}
#Comparing model7 with another model with tax rate.
# We took model 8 here which takes Crime Rate, Avg room. ,  Tax and  1/LStat which has Adj. Rsq:0.77, Cp:	8.73, BIC:-136.66	and MSE of 15.60
coef(best_subset_model, 8)
model8 <- lm(MedPrice ~ CrimeRateCat + AvgRoom + Tax + I(1/LStat), data=train)
summary(model8)
par(mfrow = c(2,2))
plot(full_model1)
ad.test(full_model1$residuals)
anova(model8)
deviance(model8)
vif(model8)




```
Findings:
The linear regression model predicting MedPrice using CrimeRateCat, AvgRoom, Tax, and 1/LStat demonstrates a strong fit to the training data, with a Multiple R-squared of 0.779 and an Adjusted R-squared of 0.7679. The model is statistically significant overall, as indicated by a high F-statistic (70.49, p < 2.2e-16).  Among the predictors, AvgRoom and 1/LStat are highly significant (p < 0.001), suggesting that homes with more rooms and a lower proportion of lower-status residents (captured by a higher value of 1/LStat) are associated with higher median housing prices.

Additionally, CrimeRateCatHigh is statistically significant (p = 0.0189), indicating that homes in high crime areas tend to have notably lower median prices compared to low crime areas. However, the Tax variable and CrimeRateCatMedium are not statistically significant (p > 0.3 and p > 0.8 respectively), suggesting weaker explanatory power in the current model context.  

The residual standard error is 3.909 on 100 degrees of freedom, and the residuals appear symmetrically distributed, as visualized in the diagnostic plots. An Anderson-Darling test on the residuals does not indicate a violation of normality assumptions. Variance Inflation Factors (VIFs) are all within acceptable ranges, indicating no significant multicollinearity concerns.  Overall, while this model is slightly weaker than Model 7, it still captures key predictors of housing prices with relatively strong explanatory power.

```{r}




#Calculate MSPE with test data, compare

# Predict MedPrice using model 7 and model 8 on test data
pred_model7 <- predict(model7, newdata = test)
pred_model8 <- predict(model8, newdata = test)


```

```{r}


# Mean Squared Prediction Error
mspe_model7 <- mean((test$MedPrice - pred_model7)^2)
mspe_model8 <- mean((test$MedPrice - pred_model8)^2)

# Print the results
mspe_model7
mspe_model8


#

```
Findings:
Model 7 performs slightly better than Model 8 on the test set as it has a lower Mean Squared Prediction Error.


Experiment building models With Removing Tax Rate outlier (666)

```{r}

#Drop outliers from train and test

cincinnati <- read_csv("CINCINNATI.csv")
cincinnati_clean <- cincinnati %>% 
  filter(Tax <= 600)
head(cincinnati_clean)
```
```{r}
#IndusCat

#There appears to be bimodality with Indus
dens <- density(cincinnati_clean$Indus)
plot(dens, main = "Checking for Bimodality in Indus")
polygon(dens, col = "skyblue", border = "darkblue")
#Find local minima
local_mins <- which(diff(sign(diff(dens$y))) == 2) + 1
min_x_values <- dens$x[local_mins]
min_y_values <- dens$y[local_mins]
#Add vertical lines and points at the local minima
abline(v = min_x_values, col = "red", lty = 2)
points(min_x_values, min_y_values, col = "red", pch = 19)
#Print the local minimum x-values
min_x_values
#Convert Indus to a categorical variable.
cincinnati_clean$IndusCat <- ifelse(cincinnati_clean$Indus < min_x_values, "Low", "High")
cincinnati_clean$IndusCat <- factor(cincinnati_clean$IndusCat, levels = c("Low", "High"))
#ggpairs(data = cincinnati_clean, aes(color = IndusCat, alpha = 0.6))

#CrimeRateCat
cincinnati_clean$CrimeRateCat <- cut(
  cincinnati_clean$CrimeRate,
  breaks = c(0, 1, 5, Inf),    # Define breakpoints for "low", "medium", "high"
  labels = c("Low", "Medium", "High")      # Labels for each category
)
cincinnati_clean$CrimeRateCat <- factor(cincinnati_clean$CrimeRateCat, levels = c("Low", "Medium", "High"))
#ggpairs(cincinnati_clean, aes(color=cincinnati_clean$CrimeRateCat, alpha=0.5))

#create binary variables for categorical variable
cincinnati_clean$IsLowCrime <- ifelse(cincinnati_clean$CrimeRateCat == "Low", 1, 0)
cincinnati_clean$IsMediumCrime <- ifelse(cincinnati_clean$CrimeRateCat == "Medium", 1, 0)
cincinnati_clean$IsHighCrime <- ifelse(cincinnati_clean$CrimeRateCat == "High", 1, 0)
```
```{r}
#train and test Set
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(cincinnati_clean), replace=TRUE, prob=c(0.7,0.3))
train_clean  <- cincinnati_clean[sample, ]
test_clean   <- cincinnati_clean[!sample, ]

```
```{r}


full_model3 <- lm(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + log(LStat), data = train_clean)
summary(full_model3)
par(mfrow = c(2,2))
plot(full_model3)
ad.test(full_model3$residuals)

full_model4 <- lm(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + I(1/LStat), data = train_clean)
summary(full_model4)
plot(full_model4)
ad.test(full_model4$residuals)

# second model (fullmodel 4 ) is performing better in this case as well.Doing bestsubset on full_model4 now 
```
```{r}
best_subset_model_clean <- regsubsets(MedPrice ~ CrimeRateCat + IndusCat + River + AvgRoom + Age + Tax + PTRatio + I(1/LStat), data = train_clean, nbest = 2)
best_subset_summary_clean <- summary(best_subset_model_clean)

#Add RSS, Adjusted R², Cp, and BIC, MSE, etc to the same data frame
mse_values <- best_subset_summary_clean$rss / (nrow(train) - 1:nrow(best_subset_summary_clean$which))
mse_values <- round(mse_values, 2)

results_clean <- data.frame(
  NumVar = 1:nrow(best_subset_summary_clean$which),
  best_subset_summary_clean$outmat,
  Rsq = best_subset_summary_clean$rsq,
  RSS = best_subset_summary_clean$rss,
  AdjR2 = best_subset_summary_clean$adjr2,
  Cp = best_subset_summary_clean$cp,
  BIC = best_subset_summary_clean$bic,
  MSE = mse_values
)
results_clean$Rsq <- round(results_clean$Rsq, 2)
results_clean$RSS <- round(results_clean$RSS, 2)
results_clean$AdjR2 <- round(results_clean$AdjR2, 2)
results_clean$Cp <- round(results_clean$Cp, 2)
results_clean$BIC <- round(results_clean$BIC, 2)


print(results_clean)
```
```{r}
# four plots for comparison

p1<-ggplot(data = results_clean, aes(x = as.factor(NumVar), y = MSE))+
  geom_point(color="black", size=2) +ylab("MSE") + xlab("No of variables")
  

p2<-ggplot(data = results_clean, aes(x = as.factor(NumVar), y = AdjR2)) +
  geom_point(color="black", size=2) +ylab("Adjusted RSQ") + xlab("No of variables")+
  geom_point(data = results_clean[which.max(results_clean$AdjR2), ], color="red", 
             size=3) 

p3<-ggplot(data = results_clean, aes(x = as.factor(NumVar), y = Cp)) +
  geom_point(color="black", size=2) +ylab("Cp") + xlab("No of variables")+
  geom_point(data = results_clean[which.min(results_clean$Cp), ], color="red", 
             size=3) 

p4<-ggplot(data = results_clean, aes(x = as.factor(NumVar), y = BIC)) +
  geom_point(color="black", size=2) +ylab("BIC") + xlab("No of variables")+
  geom_point(data = results_clean[which.min(results_clean$BIC), ], color="red", 
             size=3) 

grid.arrange(p1,p2,p3,p4,nrow=2)
```
```{r}
#Findings: Model 7 is the winner in all cases but in model 7 , predictors has changed
# CrimeRate is dropped and Tax is added as the new predictros keeping othe predictors as the same (AVGRooom, PTRatio, 1/LSAt) ,getting model 7 

coef(best_subset_model_clean, 7)
model7_clean <- lm(MedPrice ~ AvgRoom + Tax +  PTRatio + I(1/LStat), data=train_clean)
summary(model7_clean)
par(mfrow = c(2,2))
plot(model7_clean)
ad.test(model7_clean$residuals)
anova(model7_clean)
deviance(model7_clean)
vif(model7_clean)

#Findings : 82.5% of the variation is explained by these four predictors , better than before dropping tax,which was 78.5% 
```
```{r}
#Linear Regression - Forward Selection

null_model_clean <- lm(MedPrice ~ 1, data = train_clean)
upper_model_clean <- formula(full_model4)
forward_model_clean <- step(null_model_clean, direction = "forward", scope = upper_model_clean)
coef(forward_model_clean)
summary(forward_model_clean)
par(mfrow = c(2,2))
plot(forward_model_clean)
ad.test(forward_model_clean$residuals)
anova(forward_model_clean)
deviance(forward_model_clean)
vif(forward_model_clean)
```
```{r}
#Backward Selection
backward_model_clean <- step(full_model2, direction = "backward")
coef(backward_model_clean)
summary(backward_model_clean)
par(mfrow = c(2,2))
plot(backward_model_clean)
ad.test(backward_model_clean$residuals)
anova(backward_model_clean)
deviance(backward_model_clean)
vif(backward_model_clean)

#Findings: Both Forward and Backward selection produced the exact ouput as model 7 that we got from best subset.
```

Analysis
----------
When we dropped the Tax at 666 and did the building process again, we found that the model it picks is the same which is model 7 with 4 predictors but the predictors has changed where CrimeRate is dropped and Tax is added as the predictor where AvgRoom , PTRatio and 1/LSat is the same. The model with the tax dropped is slightly better as the adjR^2 is 3% more in the model where we have dropped the tax outliers.


#Notes
4/16/2025
Start with focus on demography.
Figure out how much of the houses are on the river. See if there are not many on the river, like 6 or so, if so, they may be outliers and can be dropped. 
We only have 7 houses on the river, these might destory our analysis.
Make a boxplot to check for outliers. If so, check to see if those prices are riverfront. If so, drop them. If not, keep them in.

Next, check age. 
We have a range from 3-100. Google and see what the cutoffs are, break it into categories.

Plot only y = B0 + B1x1. Make a bunch of residual plots to see if the model looks good. If several have a non constant variance , transform y variable using ln(y). Then, check to see if ln(y) = B0 + B1x1
If we see a curve indicating non linearity, add x^2 or whatever to give y = B0 + B1x1 + B2X^2. The VIF here will be huge but that doesn't matter.

Everytime we do changes, we need to fit and check the residuals and see if it improves. 

Once we decide on all the transformations, we start building. Not just best subsets, but also forward and backaward selection. Check if these models are in the best subset.

To check interactions with categorical variables, we would make a seperate chart for each option in the categorical variable to check each option to see if it interacted. In our case, plot Riverfront x1 and x2 vs medPrice. 

It is very hard to find interaction in quantitative varables. We can cateorgize it and make plots out of it to check. 
